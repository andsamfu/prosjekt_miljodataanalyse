# src/ â€“ Kildekode

Denne mappen inneholder all kildekode knyttet til prosjektets kjernelogikk. Koden er delt opp i tre hovedmoduler:

- `data_collection/` â€“ Henter inn miljÃ¸data fra Ã¥pne API-er
- `data_cleaning/` â€“ Renser og strukturerer data for analyse
- `graph/` â€“ Lager visualiseringer basert pÃ¥ analyserte data

Funksjonene her importeres og brukes i hovednotebooken, slik at sluttbrukeren kan fokusere pÃ¥ Ã¥ utforske og analysere data â€“ uten Ã¥ mÃ¥tte forholde seg til store mengder kode.

---

## ğŸ“¦ `data_collection/` â€“ Datainnsamling

Denne modulen samler inn miljÃ¸data fra to Ã¥pne API-er: Meteorologisk institutts **Frost API** og **NILU API** for luftkvalitet. Dataene hentes med Python og lagres i `data/raw/` som `.json`- eller `.db`-filer.

---

### ğŸŒ¦ï¸ Frost API (Meteorologisk institutt)

Vi bruker Frost API til Ã¥ hente vÃ¦rdata for Trondheim i perioden 2010â€“2019:

- Gjennomsnittlig lufttemperatur (daglig)
- Total nedbÃ¸r (daglig)
- Gjennomsnittlig vindstyrke (daglig)

FremgangsmÃ¥te:
- Henter nÃ¦rmeste vÃ¦rstasjon basert pÃ¥ geografiske koordinater (Trondheim sentrum)
- Henter observasjoner for valgt tidsperiode via `observations`-endepunktet
- Strukturerer dataene i en DataFrame og aggregerer dem med SQL-spÃ¸rring
- Lagrer resultatet i en SQLite-database (`data/raw/raw_api_frost_weather_trondheim_2010_to_2019.db`)

---

### ğŸŒ«ï¸ NILU API (Norsk institutt for luftforskning)

Vi bruker NILU API for Ã¥ hente luftkvalitetsdata for Trondheim for perioden 2010â€“2024.

Komponentene inkluderer blant annet:
- PM2.5
- PM10
- NOâ‚‚, Oâ‚ƒ m.m.

FremgangsmÃ¥te:
- Bruker `stats/day`-endepunktet med koordinater og radius
- Henter data for alle tilgjengelige komponenter
- Lagrer resultatet som `.json` i `data/raw/raw_api_nilu_air_quality_trondheim_2010_to_2024.json`

---

### ğŸ” NÃ¸kkelhÃ¥ndtering

- Frost API krever klient-ID (API-nÃ¸kkel), som lastes inn fra `.env`
- NILU API er offentlig og krever ingen autentisering

---

## ğŸ§¹ Datarensing (`data_cleaning/`)

Etter at rÃ¥data er hentet, behandles de i `data_cleaning/` for Ã¥ sikre at de er klare til analyse og visualisering. Dette innebÃ¦rer en rekke rensetrinn som er nÃ¸ye tilpasset formÃ¥let med prosjektet: Ã¥ sikre hÃ¸y datakvalitet, pÃ¥litelighet og sporbarhet.

### Rensetrinn og valg

Rensingen av NILU-data fÃ¸lger en strukturert prosess der hvert steg er begrunnet:

1. **Konvertering av datoer**  
   - `dateTime` konverteres til korrekt datetime-format for videre tidsseriebehandling.

2. **Pivottabell**  
   - MÃ¥linger transformeres slik at hver luftkomponent (f.eks. NOâ‚‚, PM10, PM2.5) fÃ¥r sin egen kolonne.

3. **Fjerning av uÃ¸nsket kolonne**  
   - **Valg**: *Benzo(a)pyrene in PM10 (aerosol)* fjernes.  
   - **Begrunnelse**: Denne komponenten er ikke relevant for analysen og fjernes for Ã¥ redusere stÃ¸y og holde fokus pÃ¥ sentrale luftkvalitetsindikatorer.

4. **Reindeksering for Ã¥ inkludere alle datoer**  
   - Datoindeksen utvides slik at alle datoer i perioden dekkes, ogsÃ¥ de uten mÃ¥linger.  
   - **Begrunnelse**: Dette sikrer en komplett tidsserie og tydeliggjÃ¸r eventuelle manglende data.

5. **KNN-imputasjon for manglende verdier**  
   - **Valg**: Manglende verdier fylles inn ved hjelp av KNN-imputasjon med 50 nÃ¦rmeste naboer (k=50).  
   - **Begrunnelse**: KNN gir mer nÃ¸yaktige estimater enn enklere metoder som lineÃ¦r interpolasjon, sÃ¦rlig nÃ¥r store dataintervaller mangler. Metoden tar hensyn til sammenhenger mellom komponenter og fanger opp sesongvariasjoner, noe som gir mer realistiske og pÃ¥litelige verdier.

6. **Markering av genererte verdier**  
   - Kolonner pÃ¥ formen `generated_<column>` opprettes for Ã¥ vise hvilke verdier som er imputert.  
   - **Begrunnelse**: Dette Ã¸ker transparens og gir mulighet til Ã¥ skille mellom originale og estimerte data.

7. **HÃ¥ndtering av negative verdier**  
   - **Valg**: Alle negative verdier settes til 0.  
   - **Begrunnelse**: Negative verdier er ugyldige for luftkvalitetsmÃ¥linger og kan skyldes mÃ¥lefeil.

8. **Fjerning av duplikater**  
   - **Valg**: Kun fÃ¸rste forekomst beholdes ved duplikate tidsstempler.  
   - **Begrunnelse**: Duplikater kan fÃ¸re til skjevheter og overrepresentasjon i analysen.

9. **Runding av verdier**  
   - **Valg**: Verdier rundes av til maksimalt 4 desimaler.  
   - **Begrunnelse**: Gir konsistent formatering og Ã¸kt lesbarhet, uten Ã¥ ofre nÃ¸dvendig presisjon.

10. **Lagring av renset data**  
    - **Valg**: Renset datasett lagres som `.json` i `data/clean/`.  
    - **Begrunnelse**: JSON er bÃ¥de lett Ã¥ lese og kompatibelt med videre analyse- og visualiseringsverktÃ¸y. Hver fil inneholder Ã©n rad per dato med standardiserte kolonnenavn.

---

### âœ… Oppsummering

Databehandlingen i `data_cleaning_nilu.py` fÃ¸lger prinsipper for god datarensing: fjerning av stÃ¸y, hÃ¥ndtering av manglende eller ugyldige verdier, tydelig merking av estimerte data og standardisering. Disse grepene danner et solid grunnlag for videre analyse, prediksjon og visualisering, og sikrer at dataene er bÃ¥de pÃ¥litelige og transparente.


## ğŸ“Š `graph/` â€“ Visualisering (under utvikling)

Denne modulen er forelÃ¸pig under utvikling og vil bli utvidet i neste del av prosjektet. Her skal vi lage funksjoner for Ã¥ visualisere analyseresultatene pÃ¥ en ryddig og informativ mÃ¥te.

Planen er Ã¥ bruke biblioteker som:
- **matplotlib** og **seaborn** for klassiske statiske grafer
- **plotly** for interaktive grafer i Jupyter Notebook

Visualiseringene skal gjÃ¸re det lettere Ã¥:
- Se trender og variasjoner over tid
- Sammenligne ulike miljÃ¸parametere
- Formidle innsikt basert pÃ¥ de analyserte dataene

Funksjonene i denne mappen vil kunne importeres direkte i notebooken og brukes med rensede datasett.

---

## ğŸ“Œ Merk

- API-nÃ¸kler hÃ¥ndteres med `.env`-fil og `python-dotenv`
- RÃ¥data og renset data lagres i egne undermapper for sporbarhet
- Koden er strukturert for Ã¥ kunne gjenbrukes og testes senere i prosjektet

