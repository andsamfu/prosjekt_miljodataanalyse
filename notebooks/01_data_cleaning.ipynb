{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665f2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Get path\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.append(os.path.join(project_root, 'src'))\n",
    "\n",
    "from data_cleaning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a0e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JSON-filen ble lastet vellykket.\n",
      "\n",
      "Dataset informasjon:\n",
      "\n",
      "Antall rader i datasettet: 5222\n",
      "\n",
      "Genererte verdier per kolonne:\n",
      "  - generated_NO2: 1268 genererte verdier\n",
      "  - generated_PM10: 524 genererte verdier\n",
      "  - generated_PM2.5: 532 genererte verdier\n",
      "\n",
      "Antall outliers fjernet per kolonne:\n",
      "  - NO2: referenceTime\n",
      "2010-01-07 00:00:00+01:00     74.091667\n",
      "2010-01-13 00:00:00+01:00     75.187500\n",
      "2010-01-14 00:00:00+01:00     77.004167\n",
      "2010-01-15 00:00:00+01:00    106.983333\n",
      "2010-02-02 00:00:00+01:00     84.012500\n",
      "2010-02-03 00:00:00+01:00     78.891667\n",
      "2010-11-17 00:00:00+01:00     75.575000\n",
      "2010-11-19 00:00:00+01:00     80.945833\n",
      "2010-12-23 00:00:00+01:00     74.150000\n",
      "2011-01-04 00:00:00+01:00     91.450000\n",
      "2013-01-29 00:00:00+01:00     80.120833\n",
      "2013-12-09 00:00:00+01:00     76.295833\n",
      "2014-12-18 00:00:00+01:00     82.083333\n",
      "dtype: float64 outliers fjernet\n",
      "  - PM10: referenceTime\n",
      "2010-01-07 00:00:00+01:00    54.004167\n",
      "2010-01-13 00:00:00+01:00    53.500000\n",
      "2010-01-14 00:00:00+01:00    67.504167\n",
      "2010-01-15 00:00:00+01:00    72.045833\n",
      "2010-01-23 00:00:00+01:00    53.454167\n",
      "2010-01-31 00:00:00+01:00    65.370833\n",
      "2010-02-01 00:00:00+01:00    50.520833\n",
      "2010-02-02 00:00:00+01:00    64.520833\n",
      "2010-02-03 00:00:00+01:00    47.979167\n",
      "2010-04-06 00:00:00+01:00    48.745833\n",
      "2010-04-07 00:00:00+01:00    51.500000\n",
      "2010-04-12 00:00:00+01:00    55.700000\n",
      "2010-11-19 00:00:00+01:00    58.020833\n",
      "2010-11-28 00:00:00+01:00    57.995833\n",
      "2010-12-21 00:00:00+01:00    58.450000\n",
      "2010-12-22 00:00:00+01:00    69.304167\n",
      "2010-12-23 00:00:00+01:00    72.933333\n",
      "2010-12-24 00:00:00+01:00    55.679167\n",
      "2011-01-04 00:00:00+01:00    47.520833\n",
      "2011-02-12 00:00:00+01:00    52.720833\n",
      "2011-03-15 00:00:00+01:00    57.762500\n",
      "2011-03-16 00:00:00+01:00    48.983333\n",
      "2011-03-17 00:00:00+01:00    57.041667\n",
      "2011-04-13 00:00:00+01:00    64.220833\n",
      "2011-04-14 00:00:00+01:00    48.120833\n",
      "2011-04-28 00:00:00+01:00    51.116667\n",
      "2011-10-24 00:00:00+01:00    55.183333\n",
      "2011-11-09 00:00:00+01:00    55.304167\n",
      "2012-01-20 00:00:00+01:00    52.895833\n",
      "2012-01-23 00:00:00+01:00    51.391667\n",
      "2012-01-30 00:00:00+01:00    66.800000\n",
      "2012-01-31 00:00:00+01:00    64.033333\n",
      "2012-02-01 00:00:00+01:00    58.162500\n",
      "2012-03-05 00:00:00+01:00    72.375000\n",
      "2012-03-06 00:00:00+01:00    53.783333\n",
      "2012-12-05 00:00:00+01:00    47.054167\n",
      "2013-11-07 00:00:00+01:00    81.783333\n",
      "2014-01-21 00:00:00+01:00    62.387500\n",
      "2014-03-26 00:00:00+01:00    55.316667\n",
      "2015-03-18 00:00:00+01:00    47.650000\n",
      "2016-01-05 00:00:00+01:00    61.783333\n",
      "2016-01-14 00:00:00+01:00    61.479167\n",
      "2017-02-09 00:00:00+01:00    74.845833\n",
      "2019-04-25 00:00:00+01:00    87.441667\n",
      "2019-04-26 00:00:00+01:00    48.345833\n",
      "2020-10-02 00:00:00+01:00    59.516667\n",
      "2020-10-03 00:00:00+01:00    68.979167\n",
      "2022-03-14 00:00:00+01:00    57.683333\n",
      "2022-11-18 00:00:00+01:00    59.891667\n",
      "2023-11-15 00:00:00+01:00    50.964300\n",
      "2024-01-31 00:00:00+01:00    60.054167\n",
      "2024-03-12 00:00:00+01:00    48.595833\n",
      "dtype: float64 outliers fjernet\n",
      "  - PM2.5: referenceTime\n",
      "2010-01-01 00:00:00+01:00    38.704167\n",
      "2010-01-02 00:00:00+01:00    30.520833\n",
      "2010-01-06 00:00:00+01:00    37.825000\n",
      "2010-01-07 00:00:00+01:00    51.837500\n",
      "2010-01-08 00:00:00+01:00    34.854167\n",
      "2010-01-13 00:00:00+01:00    41.512500\n",
      "2010-01-14 00:00:00+01:00    53.637500\n",
      "2010-01-15 00:00:00+01:00    68.625000\n",
      "2010-01-23 00:00:00+01:00    45.616667\n",
      "2010-01-31 00:00:00+01:00    62.766667\n",
      "2010-02-01 00:00:00+01:00    47.962500\n",
      "2010-02-02 00:00:00+01:00    57.775000\n",
      "2010-02-03 00:00:00+01:00    43.591667\n",
      "2010-11-17 00:00:00+01:00    34.541667\n",
      "2010-11-18 00:00:00+01:00    36.870833\n",
      "2010-11-19 00:00:00+01:00    48.675000\n",
      "2010-11-28 00:00:00+01:00    55.291667\n",
      "2010-11-29 00:00:00+01:00    33.516667\n",
      "2010-12-21 00:00:00+01:00    51.100000\n",
      "2010-12-22 00:00:00+01:00    61.662500\n",
      "2010-12-23 00:00:00+01:00    70.108333\n",
      "2010-12-24 00:00:00+01:00    54.683333\n",
      "2011-01-04 00:00:00+01:00    43.612500\n",
      "2011-11-09 00:00:00+01:00    32.895833\n",
      "2011-11-10 00:00:00+01:00    31.054167\n",
      "2012-01-01 00:00:00+01:00    31.070833\n",
      "2012-01-07 00:00:00+01:00    40.841667\n",
      "2012-01-23 00:00:00+01:00    30.470833\n",
      "2012-01-30 00:00:00+01:00    34.812500\n",
      "2012-01-31 00:00:00+01:00    41.350000\n",
      "2012-02-01 00:00:00+01:00    42.579167\n",
      "2012-12-06 00:00:00+01:00    35.504167\n",
      "2012-12-07 00:00:00+01:00    36.310000\n",
      "2012-12-08 00:00:00+01:00    29.608333\n",
      "2012-12-09 00:00:00+01:00    31.408333\n",
      "2014-12-17 00:00:00+01:00    29.570833\n",
      "2016-01-14 00:00:00+01:00    34.075000\n",
      "2018-01-19 00:00:00+01:00    31.295833\n",
      "2018-01-20 00:00:00+01:00    39.416667\n",
      "2018-01-22 00:00:00+01:00    29.479167\n",
      "2018-12-15 00:00:00+01:00    29.750000\n",
      "2020-10-03 00:00:00+01:00    31.916667\n",
      "2021-01-07 00:00:00+01:00    30.045833\n",
      "2021-12-06 00:00:00+01:00    29.462500\n",
      "2021-12-07 00:00:00+01:00    34.416667\n",
      "dtype: float64 outliers fjernet\n",
      "\n",
      "Konverterer data til JSON format...\n",
      "Renset data lagret i 'h:\\my-documents\\01-projects\\00124-prosjekt_oppgave\\data\\clean\\cleaned_data_nilu.json'\n",
      "\n",
      "Data rensing fullf√∏rt\n"
     ]
    }
   ],
   "source": [
    "main_dc_nilu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db8772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values detected:\n",
      "mean_wind_speed:\n",
      "- 2019: 1\n",
      "\n",
      "No outliers detected\n",
      "\n",
      "Date gaps detected:\n",
      "- 2012: 199\n",
      "\n",
      "Generated values:\n",
      "- mean_air_temperature: 199\n",
      "- mean_wind_speed: 200\n",
      "- total_precipitation: 199\n",
      "\n",
      "Cleaned data saved to 'h:\\my-documents\\01-projects\\00124-prosjekt_oppgave\\data\\clean\\frost.db' in the table 'weather_data'.\n"
     ]
    }
   ],
   "source": [
    "default_clean_frost_data(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517471da",
   "metadata": {},
   "source": [
    "# üßº Rensing og klargj√∏ring av data\n",
    "\n",
    "I denne notebooken klargj√∏r vi de innhentede datasettene fra NILU og Frost for videre analyse og visualisering.\n",
    "\n",
    "Vi utf√∏rer:\n",
    "- Strukturering av data\n",
    "- Fjerning av feil og outliers\n",
    "- Behandling av manglende verdier\n",
    "\n",
    "All renselogikk ligger i egne funksjoner og moduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed485a2d",
   "metadata": {},
   "source": [
    "## üßΩ Om rensing og datakvalitet\n",
    "\n",
    "Etter at vi har hentet inn data fra Frost (MET) og NILU, er neste steg √• gj√∏re dataene klare til analyse. Dette handler b√•de om √• forbedre kvaliteten p√• dataene ‚Äì og om √• vise at vi forst√•r hvordan ulike datakilder stiller ulike krav til rensing.\n",
    "\n",
    "Vi valgte to datasett med kontrasterende egenskaper:\n",
    "\n",
    "- **Frost**: Strukturert, stabilt og nesten ¬´analyseklart¬ª\n",
    "- **NILU**: Ujevnt, med mange manglende verdier og m√•leutfordringer\n",
    "\n",
    "Denne kontrasten gir oss mulighet til √• vise b√•de enkel validering og mer avansert rensing og imputasjon.\n",
    "\n",
    "---\n",
    "\n",
    "### üå´Ô∏è NILU ‚Äì utfordrende, men verdifullt\n",
    "\n",
    "Datasettet fra NILU inneholder daglige m√•linger av luftkvalitet i Trondheim (PM10, PM2.5, NO‚ÇÇ), men er preget av:\n",
    "\n",
    "- Mange manglende verdier\n",
    "- Ekstreme m√•linger og st√∏y\n",
    "- Ujevn dekning over tid og mellom komponenter\n",
    "\n",
    "Samtidig er dette realistiske utfordringer ved milj√∏data, og gir oss muligheten til √• vise god databehandling i praksis.\n",
    "\n",
    "#### üîß Renseprosessen\n",
    "\n",
    "1. **Konvertering av datoer**  \n",
    "`dateTime` ble konvertert til `datetime`-format for √• gj√∏re videre tidsseriebehandling mulig (f.eks. resampling og glatting).\n",
    "\n",
    "2. **Pivotering av datastruktur**  \n",
    "M√•lingene ble transformert slik at √©n rad tilsvarer √©n dag, og hver luftkomponent fikk sin egen kolonne. Dette forenkler all videre analyse.\n",
    "\n",
    "3. **Fjerning av kolonne**  \n",
    "`Benzo(a)pyrene in PM10` ble fjernet fordi den hadde nesten bare manglende verdier, og ikke er relevant for v√•r problemstilling.\n",
    "\n",
    "4. **Fjerning av outliers**  \n",
    "M√•linger som l√• mer enn 4 standardavvik fra gjennomsnittet ble fjernet. Disse kan v√¶re feil eller kortvarige, ekstreme hendelser som ikke representerer typiske forhold.\n",
    "\n",
    "5. **Reindeksering av datoer**  \n",
    "Alle datoer mellom f√∏rste og siste registrering ble inkludert, ogs√• de uten m√•linger. Dette gj√∏r eventuelle datamangler synlige og muliggj√∏r presis imputasjon.\n",
    "\n",
    "6. **KNN-imputasjon (k = 100)**  \n",
    "Manglende verdier ble fylt inn med KNN-imputasjon, som bruker lignende dager til √• estimere manglende verdier. Mer om KNN-impulasjon og hvorfor vi har valgt dette kan du lese [her](KNN_imputation.ipynb)\n",
    "\n",
    "\n",
    "7. **Merking av estimerte verdier**  \n",
    "Vi opprettet egne `generated_*`-kolonner for √• vise hvilke verdier som er estimert. Dette gj√∏r datagrunnlaget transparent og analyserbart.\n",
    "\n",
    "8. **Glidende gjennomsnitt**  \n",
    "Vi brukte et 3-dagers glidende gjennomsnitt for √• jevne ut tilfeldige variasjoner og fremheve trender.  \n",
    "‚Äì *Hvorfor?* Det gir mer lesbare grafer og et tydeligere bilde av utvikling, men uten √• skjule kortsiktige endringer.\n",
    "\n",
    "9. **Negative verdier satt til 0**  \n",
    "Luftforurensningsverdier kan ikke v√¶re negative. Disse feilene ble rettet for √• bevare datasettets troverdighet.\n",
    "\n",
    "10. **Lagring som JSON**  \n",
    "Renset data ble lagret som `.json`.  \n",
    "‚Äì Lett √• lese og bruke videre i b√•de analyse og visualisering.\n",
    "\n",
    "---\n",
    "\n",
    "### üå¶Ô∏è Frost ‚Äì strukturert, men fortsatt valideringsbehov\n",
    "\n",
    "Frost-dataene er langt mer komplette og standardiserte, men ble likevel validert og bearbeidet for √• sikre p√•litelighet ‚Äì og for √• kunne kombineres med NILU.\n",
    "\n",
    "#### üîß Renseprosessen\n",
    "\n",
    "1. **Konvertering av datoformat**  \n",
    "`referenceTime` ble formatert til datoobjekt for √• kunne brukes i tidsserier.\n",
    "\n",
    "2. **Utvalg av relevante variabler**  \n",
    "Vi fokuserte p√•:\n",
    "  - Temperatur (gjennomsnitt per dag)\n",
    "  - Nedb√∏r (total per dag)\n",
    "  - Vindhastighet (gjennomsnitt per dag)\n",
    "\n",
    "  Disse har dokumentert sammenheng med luftkvalitet. Andre mer usikre eller tekniske variabler ble utelatt.\n",
    "\n",
    "3. **Pivotering og navneendring**  \n",
    "Datasettet ble omstrukturert til √©n rad per dag med mer intuitive kolonnenavn.\n",
    "\n",
    "4. **Verdikontroll med `ValueRangeValidator`**  \n",
    "Vi definerte gyldige verdier for Trondheim:\n",
    "  - Temp: -30 til 40‚ÄØ¬∞C  \n",
    "  - Nedb√∏r: 0 til 250‚ÄØmm  \n",
    "  - Vind: 0 til 60‚ÄØm/s  \n",
    "\n",
    "  Verdier utenfor ble fjernet ‚Äì de er trolig feilregistreringer.\n",
    "\n",
    "5. **Kontroll for dato-hull**  \n",
    "Med `DateContinuityValidator` sjekket vi at det ikke manglet perioder i tidsserien. Sm√• hull ble registrert, men ikke imputert ‚Äì siden Frost generelt har god datadekning.\n",
    "\n",
    "6. **Lagring som SQLite**  \n",
    "V√¶rdataene ble lagret i en SQLite-database (`frost.db`) ‚Äì ideelt for videre analyse med SQL og for eventuell samkj√∏ring med st√∏rre datasett.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Forutsetninger for datarensing\n",
    "\n",
    "F√∏r vi kan starte datarensingen, m√• vi ha tilgang til r√•dataene. Disse hentes ved √•:\n",
    "\n",
    "1. Kj√∏re `data_collection.ipynb` notebooken f√∏rst\n",
    "2. Velge tidsperiode og by for datainnsamling\n",
    "3. La API-kallene hente data fra Frost og NILU\n",
    "4. Dataene lagres automatisk i:\n",
    "   - `data/raw/api_frost_weather.json`\n",
    "   - `data/raw/api_nilu_air_quality.json`\n",
    "\n",
    "N√•r disse filene er p√• plass, kan vi starte renseprosessen.\n",
    "\n",
    "---\n",
    "\n",
    "### üå¶Ô∏è Rensing av Frost v√¶rdata\n",
    "\n",
    "F√∏rst skal vi rense v√¶rdataene fra Frost API. Disse dataene er relativt strukturerte og komplette, men vi m√• fortsatt:\n",
    "\n",
    "1. Validere at alle felt har gyldige verdier innenfor forventede grenser\n",
    "2. Sjekke for manglende verdier og datoer\n",
    "3. H√•ndtere eventuelle outliers\n",
    "4. Lagre resultatet i en SQL database for videre analyse\n",
    "\n",
    "Vi bruker funksjonen `clean_frost_data()` som h√•ndterer hele renseprosessen gjennom en serie validatorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default paths\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get paths relative to the notebook directory\n",
    "project_root = Path(os.getcwd()).parent\n",
    "raw_frost_json = project_root / 'data' / 'raw' / 'api_frost_weather.json'\n",
    "clean_frost_db = project_root / 'data' / 'clean' / 'frost.db'\n",
    "\n",
    "# First check if source and destination paths exist\n",
    "if not raw_frost_json.exists():\n",
    "    print(f\"\\nError: Source file not found: {raw_frost_json}\")\n",
    "    print(\"\\nYou need to run data collection first to create the input file.\")\n",
    "    print(\"Please run the data_collection.ipynb notebook first to fetch data from Frost API.\")\n",
    "else:\n",
    "    print(f\"Found source file: {raw_frost_json}\")\n",
    "    \n",
    "    # Create destination directory if it doesn't exist\n",
    "    clean_frost_db.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nStarting Frost data cleaning process...\")\n",
    "    print(f\"Reading from: {raw_frost_json}\")\n",
    "    print(f\"Will save to: {clean_frost_db}\\n\")\n",
    "    \n",
    "    # Clean Frost data and save to SQLite database\n",
    "    clean_frost_data(str(raw_frost_json), str(clean_frost_db))\n",
    "    \n",
    "    if clean_frost_db.exists():\n",
    "        print(f\"\\nFrost data cleaning completed.\")\n",
    "        print(f\"Results saved to: {clean_frost_db}\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: The cleaning process did not create the expected output file!\")\n",
    "        print(f\"Expected file: {clean_frost_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c50e0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üå´Ô∏è Rensing av NILU luftkvalitetsdata\n",
    "\n",
    "N√• skal vi rense luftkvalitetsdataene fra NILU API. Som beskrevet over, er dette datasettet mer utfordrende med:\n",
    "\n",
    "- Manglende verdier som m√• h√•ndteres\n",
    "- Outliers som m√• identifiseres og fjernes\n",
    "- Data som m√• restruktureres for analyse\n",
    "\n",
    "Vi bruker funksjonen `main_dc_nilu()` som implementerer alle stegene i renseprosessen beskrevet over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default paths\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get paths relative to the notebook directory\n",
    "project_root = Path(os.getcwd()).parent\n",
    "raw_nilu_json = project_root / 'data' / 'raw' / 'api_nilu_air_quality.json'\n",
    "clean_nilu_json = project_root / 'data' / 'clean' / 'cleaned_data_nilu.json'\n",
    "\n",
    "# Check if source and destination paths exist for NILU data\n",
    "if not raw_nilu_json.exists():\n",
    "    print(f\"\\nError: Source file not found: {raw_nilu_json}\")\n",
    "    print(\"\\nYou need to run data collection first to create the input file.\")\n",
    "    print(\"Please run the data_collection.ipynb notebook first to fetch data from NILU API.\")\n",
    "else:\n",
    "    print(f\"Found source file: {raw_nilu_json}\")\n",
    "    \n",
    "    # Create destination directory if it doesn't exist\n",
    "    clean_nilu_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nStarting NILU data cleaning process...\")\n",
    "    print(f\"Reading from: {raw_nilu_json}\")\n",
    "    print(f\"Will save to: {clean_nilu_json}\\n\")\n",
    "    \n",
    "    # Clean NILU data and save to JSON file\n",
    "    main_dc_nilu(str(raw_nilu_json), str(clean_nilu_json))\n",
    "    \n",
    "    if clean_nilu_json.exists():\n",
    "        print(f\"\\nNILU data cleaning completed.\")\n",
    "        print(f\"Results saved to: {clean_nilu_json}\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: The cleaning process did not create the expected output file!\")\n",
    "        print(f\"Expected file: {clean_nilu_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467b0c4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üß† Hvorfor dette er en god tiln√¶rming\n",
    "\n",
    "Vi har valgt rensestrategier basert p√• datakildenes egenskaper:\n",
    "\n",
    "- **NILU** krevde:\n",
    "  - Komplett rekonstruering av datastrukturen\n",
    "  - Avansert imputasjon og glatting\n",
    "  - Transparens rundt hvilke verdier som er estimert\n",
    "\n",
    "- **Frost** krevde:\n",
    "  - Kontroll av gyldige verdier og datoer\n",
    "  - Lett justering og tilrettelegging for videre analyse\n",
    "\n",
    "Dette viser:\n",
    "- Forst√•else av hva som p√•virker datakvalitet\n",
    "- Evne til √• tilpasse metoder etter utfordring\n",
    "- Fokus p√• transparens og sporbarhet\n",
    "\n",
    "Resultatet er et datasett som b√•de er **ryddet og dokumentert**, og klart for videre analyse, visualisering og modellering.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Neste steg: Analyse og visualisering\n",
    "\n",
    "N√• som begge datasett er renset og klargjort, er vi klare for √• utforske innholdet mer inng√•ende. I neste notebook skal vi:\n",
    "\n",
    "- Beregne statistiske m√•l som gjennomsnitt, median og standardavvik\n",
    "- Unders√∏ke sammenhenger og trender i luftkvalitet og v√¶rdata over tid\n",
    "- Lage visuelle fremstillinger som gj√∏r datam√∏nstre lettere √• forst√•\n",
    "\n",
    "\n",
    "### [**Videre til analyse og visualisering**](02_data_analysis_and_visualisation.ipynb)\n",
    "##### [**Til samlesiden**](../docs/samleside.md)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
