{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665f2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.append(os.path.join(project_root, 'src'))\n",
    "\n",
    "from data_cleaning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517471da",
   "metadata": {},
   "source": [
    "# 🧼 Rensing og klargjøring av data\n",
    "\n",
    "I denne notebooken klargjør vi de innhentede datasettene fra NILU og Frost for videre analyse og prediktiv modellering. Renseprosessen handler både om å forbedre datakvalitet, og om å tilpasse datastrukturen til videre bruk i visualisering, analyse og maskinlæring.\n",
    "\n",
    "Vi utfører blant annet:\n",
    "- Strukturering og transformasjon av data\n",
    "- Fjerning av uteliggere og urealistiske verdier\n",
    "- Behandling av manglende data\n",
    "- Tidsserietilpasning og kvalitetssikring av måleperioder\n",
    "\n",
    "All renselogikk er kapslet i egne Python-moduler for å gjøre notebooken ryddig og funksjonell. Det betyr at denne notebooken kun importerer og bruker funksjoner definert i `src/`, slik at det blir enkelt å gjenbruke og vedlikeholde koden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed485a2d",
   "metadata": {},
   "source": [
    "### Om rensing og datakvalitet\n",
    "\n",
    "Etter innhenting av rådata fra Frost (MET) og NILU, har vi gjennomført omfattende rensing for å sikre at datagrunnlaget er pålitelig og egnet for analyse og modellering. Renseprosessen har vært tilpasset egenskapene til hvert enkelt datasett.\n",
    "\n",
    "Vi valgte to datasett med ulike utfordringer:\n",
    "\n",
    "- **Frost**: Strukturert og nesten analyseklart, men med behov for validering og omstrukturering\n",
    "- **NILU**: Reelt utfordrende – med manglende verdier, uteliggere og ujevn målefrekvens\n",
    "\n",
    "Denne kontrasten har gitt oss anledning til å vise både enkel og avansert databehandling, samt reflektere rundt hvordan ulike strategier kreves for ulike datatyper.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌫️ NILU – komplekst, men realistisk\n",
    "\n",
    "Datasettet fra NILU inneholder daglige målinger for tre luftkomponenter i Trondheim. Vi valgte dette bevisst fordi det illustrerer virkelige utfordringer med miljødata, spesielt:\n",
    "\n",
    "- Inkonsekvent dekning mellom komponenter\n",
    "- Støy og uteliggere\n",
    "- Ufullstendig tidsdekning\n",
    "\n",
    "Vi utviklet en egen rensefil (`data_cleaning_nilu.py`) med tydelig struktur:\n",
    "\n",
    "### 🔧 Rensing av NILU-data – steg for steg\n",
    "\n",
    "1. **Konvertering og sortering av datoer**\n",
    "   - Vi konverterer `dateTime` til `datetime`-objekter og sorterer datasettet kronologisk.\n",
    "   - Dette er nødvendig for å kunne jobbe med dataserier og bruke KNN-imputasjon senere.\n",
    "\n",
    "2. **Fjerning av lite relevante komponenter**\n",
    "   - Kolonnen `Benzo(a)pyrene in PM10` fjernes fordi den består nesten utelukkende av manglende verdier og ikke er sentral for vår analyse.\n",
    "\n",
    "3. **Outlier-fjerning**\n",
    "   - Med `OutlierValidator` fjerner vi verdier som er mer enn 4 standardavvik fra gjennomsnittet.\n",
    "   - Dette gjør vi separat for hver komponent (NO₂, PM10, PM2.5) for å redusere påvirkning fra ekstremverdier.\n",
    "\n",
    "4. **KNN-imputasjon for manglende verdier**\n",
    "   - Vi bruker `KNNImputer` fra `sklearn` til å estimere manglende verdier.\n",
    "   - Imputasjonen baseres på lignende dager i datasettet, noe som gir mer realistiske estimater enn enkle gjennomsnitt.\n",
    "   - Du kan lese mer om dette steget [**her**](../notebooks/KNN_imputation.ipynb).\n",
    "\n",
    "5. **Merking av estimerte verdier**\n",
    "   - Vi legger til egne kolonner (`generated_NO2`, `generated_PM10`, `generated_PM2.5`) som markerer hvilke verdier som er imputert med KNN.\n",
    "   - Dette gir transparens og gjør det mulig å filtrere estimerte verdier i videre analyser.\n",
    "\n",
    "6. **Datavalidering**\n",
    "   - Vi bruker `MissingValueValidator`, `OutlierValidator` og `DateContinuityValidator` til å gi oversikt over datakvalitet før og etter rensing.\n",
    "   - Dette gir dokumentasjon av renseprosessen og innsikt i effekten av hvert steg.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🌦️ Frost – robust, men ikke perfekt\n",
    "\n",
    "Frost-dataene fra MET har høy kvalitet, men ble likevel renset og validert med `data_cleaning_frost.py`.\n",
    "\n",
    "### 🔧 Håndtering av Frost-data – steg for steg\n",
    "\n",
    "1. **Innlasting og datakonvertering**\n",
    "   - Data hentes fra en rå `.json`-fil og konverteres til et strukturert DataFrame-format.\n",
    "   - `referenceTime` konverteres til `datetime` og brukes som indeks for sortering.\n",
    "\n",
    "2. **Filtrering av ønskede måleparametere**\n",
    "   - Vi fokuserer på tre sentrale værvariabler:\n",
    "     - Daglig gjennomsnittstemperatur (`mean_air_temperature`)\n",
    "     - Daglig total nedbør (`sum_precipitation_amount`)\n",
    "     - Daglig gjennomsnittlig vindstyrke (`mean_wind_speed`)\n",
    "   - Alle andre måletyper ekskluderes.\n",
    "\n",
    "3. **Pivotering og navneforenkling**\n",
    "   - Datasettet transformeres slik at én rad representerer én dato, med egne kolonner for hver værkomponent.\n",
    "   - Variabelnavn gjøres mer lesbare og konsistente.\n",
    "\n",
    "4. **Validering og rensing av verdier**\n",
    "   - Vi bruker `ValueRangeValidator` til å fjerne urealistiske målinger:\n",
    "     - Temperatur: mellom -30 og 40 °C\n",
    "     - Nedbør: mellom 0 og 250 mm\n",
    "     - Vind: mellom 0 og 60 m/s\n",
    "   - Dette hindrer feilaktige uteliggere i å forstyrre analysen.\n",
    "\n",
    "5. **Kontinuitet i datoer**\n",
    "   - `DateContinuityValidator` kontrollerer at det ikke er større hull i tidsserien.\n",
    "   - Vi logger eventuelle manglende datoer, men imputasjon gjøres ikke da Frost-dataene er svært komplette.\n",
    "\n",
    "6. **Lagring i SQLite**\n",
    "   - Det rensede datasettet lagres i en SQLite-database (`frost.db`) for videre analyse og prediktiv modellering.\n",
    "   - Dette gjør det enkelt å koble Frost- og NILU-data senere ved hjelp av SQL.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Hvorfor disse valgene?\n",
    "\n",
    "Valgene våre er gjort med tanke på:\n",
    "- **Datakvalitet**: Vi ønsker å kunne stole på analysene\n",
    "- **Åpenhet**: Vi synliggjør estimerte data og ekstreme verdier\n",
    "- **Brukervennlighet**: Formatene vi lagrer i er lette å bruke i både Python og SQL\n",
    "- **Tilpasning til datasettene**: Frost og NILU krever ulik tilnærming – og det har vi tatt hensyn til\n",
    "\n",
    "Ved å bruke egne validator-klasser (som `MissingValueValidator`, `OutlierValidator`, `DateContinuityValidator`, `ValueRangeValidator`) har vi laget en fleksibel og gjenbrukbar løsning som også kan skaleres til andre datatyper.\n",
    "\n",
    "---\n",
    "\n",
    "Vil du se koden som gjør alt dette? Ta en titt på:\n",
    "- [`data_cleaning_nilu.py`](../src/data_cleaning/data_cleaning_nilu.py)\n",
    "- [`data_cleaning_frost.py`](../src/data_cleaning/data_cleaning_frost.py)\n",
    "- [`data_validators.py`](../src/data_cleaning/data_validators.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c50e0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🌫️ Rensing av NILU luftkvalitetsdata\n",
    "\n",
    "Nå skal vi kjøre selve koden som renser luftkvalitetsdataene fra NILU. Som beskrevet over, er dette datasettet mer utfordrende med:\n",
    "\n",
    "- Manglende verdier som må håndteres\n",
    "- Outliers som må identifiseres og fjernes\n",
    "- Data som må restruktureres for analyse\n",
    "\n",
    "Vi bruker funksjonen `main_dc_nilu()` som implementerer alle stegene i renseprosessen beskrevet over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e80811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JSON-filen ble lastet vellykket.\n",
      "\n",
      "Dataset informasjon:\n",
      "\n",
      "Antall rader i datasettet: 5222\n",
      "\n",
      "Antall outliers fjernet per verdi:\n",
      "  - NO2: 13 outliers fjernet\n",
      "  - PM10: 52 outliers fjernet\n",
      "  - PM2.5: 45 outliers fjernet\n",
      "\n",
      "Genererte verdier:\n",
      "  - generated_NO2: 1268 genererte verdier\n",
      "  - generated_PM10: 524 genererte verdier\n",
      "  - generated_PM2.5: 532 genererte verdier\n",
      "\n",
      "Konverterer data til JSON format...\n",
      "Renset data lagret i '/Users/oliverroddesnes/Documents/Anvendt_programering/prosjekt_miljodataanalyse/data/clean/cleaned_data_nilu.json'\n",
      "\n",
      "Data rensing fullført\n"
     ]
    }
   ],
   "source": [
    "main_dc_nilu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9219302",
   "metadata": {},
   "source": [
    "### 🌦️ Rensing av Frost værdata\n",
    "\n",
    "Nå skal vi rense værdataene hentet fra Frost API. Dette datasettet er langt mer strukturert enn NILU-dataene, men det krever fortsatt:\n",
    "\n",
    "- Omstrukturering for å få én rad per dag\n",
    "- Fjerning av urealistiske verdier via validering\n",
    "- Kontroll av datakontinuitet over hele måleperioden\n",
    "\n",
    "Vi bruker funksjonen `default_clean_frost_data()` som gjennomfører hele renseprosessen steg for steg – slik det er beskrevet over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91b26f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values detected:\n",
      "mean_wind_speed:\n",
      "- 2012: 1\n",
      "- 2019: 1\n",
      "\n",
      "No outliers detected\n",
      "\n",
      "No date gaps detected\n",
      "\n",
      "Generated values:\n",
      "- mean_wind_speed: 2\n",
      "\n",
      "Cleaned data saved to '/Users/oliverroddesnes/Documents/Anvendt_programering/prosjekt_miljodataanalyse/data/clean/frost.db' in the table 'weather_data'.\n"
     ]
    }
   ],
   "source": [
    "default_clean_frost_data(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467b0c4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🧠 Hvorfor dette er en god tilnærming\n",
    "\n",
    "Vi har valgt rensestrategier basert på datakildenes egenskaper:\n",
    "\n",
    "- **NILU** krevde:\n",
    "  - Komplett rekonstruering av datastrukturen\n",
    "  - Avansert imputasjon og glatting\n",
    "  - Transparens rundt hvilke verdier som er estimert\n",
    "\n",
    "- **Frost** krevde:\n",
    "  - Kontroll av gyldige verdier og datoer\n",
    "  - Lett justering og tilrettelegging for videre analyse\n",
    "\n",
    "Dette viser:\n",
    "- Forståelse av hva som påvirker datakvalitet\n",
    "- Evne til å tilpasse metoder etter utfordring\n",
    "- Fokus på transparens og sporbarhet\n",
    "\n",
    "Resultatet er et datasett som både er **ryddet og dokumentert**, og klart for videre analyse, visualisering og modellering.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Neste steg: Analyse og visualisering\n",
    "\n",
    "Nå som begge datasett er renset og klargjort, er vi klare for å utforske innholdet mer inngående. I neste notebook skal vi:\n",
    "\n",
    "- Beregne statistiske mål som gjennomsnitt, median og standardavvik\n",
    "- Undersøke sammenhenger og trender i luftkvalitet og værdata over tid\n",
    "- Lage visuelle fremstillinger som gjør datamønstre lettere å forstå\n",
    "\n",
    "\n",
    "### [**Videre til analyse og visualisering**](02_data_analysis_and_visualisation.ipynb)\n",
    "##### [**Til samlesiden**](../docs/samleside.md)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
