{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665f2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.append(os.path.join(project_root, 'src'))\n",
    "\n",
    "from data_cleaning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517471da",
   "metadata": {},
   "source": [
    "# ğŸ§¼ Rensing og klargjÃ¸ring av data\n",
    "\n",
    "I denne notebooken klargjÃ¸r vi de innhentede datasettene fra NILU og Frost for videre analyse og prediktiv modellering. Renseprosessen handler bÃ¥de om Ã¥ forbedre datakvalitet, og om Ã¥ tilpasse datastrukturen til videre bruk i visualisering, analyse og maskinlÃ¦ring.\n",
    "\n",
    "Vi utfÃ¸rer blant annet:\n",
    "- Strukturering og transformasjon av data\n",
    "- Fjerning av uteliggere og urealistiske verdier\n",
    "- Behandling av manglende data\n",
    "- Tidsserietilpasning og kvalitetssikring av mÃ¥leperioder\n",
    "\n",
    "All renselogikk er kapslet i egne Python-moduler for Ã¥ gjÃ¸re notebooken ryddig og funksjonell. Det betyr at denne notebooken kun importerer og bruker funksjoner definert i `src/`, slik at det blir enkelt Ã¥ gjenbruke og vedlikeholde koden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed485a2d",
   "metadata": {},
   "source": [
    "### Om rensing og datakvalitet\n",
    "\n",
    "Etter innhenting av rÃ¥data fra Frost (MET) og NILU, har vi gjennomfÃ¸rt omfattende rensing for Ã¥ sikre at datagrunnlaget er pÃ¥litelig og egnet for analyse og modellering. Renseprosessen har vÃ¦rt tilpasset egenskapene til hvert enkelt datasett.\n",
    "\n",
    "Vi valgte to datasett med ulike utfordringer:\n",
    "\n",
    "- **Frost**: Strukturert og nesten analyseklart, men med behov for validering og omstrukturering\n",
    "- **NILU**: Reelt utfordrende â€“ med manglende verdier, uteliggere og ujevn mÃ¥lefrekvens\n",
    "\n",
    "Denne kontrasten har gitt oss anledning til Ã¥ vise bÃ¥de enkel og avansert databehandling, samt reflektere rundt hvordan ulike strategier kreves for ulike datatyper.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ«ï¸ NILU â€“ komplekst, men realistisk\n",
    "\n",
    "Datasettet fra NILU inneholder daglige mÃ¥linger for tre luftkomponenter i Trondheim. Vi valgte dette bevisst fordi det illustrerer virkelige utfordringer med miljÃ¸data, spesielt:\n",
    "\n",
    "- Inkonsekvent dekning mellom komponenter\n",
    "- StÃ¸y og uteliggere\n",
    "- Ufullstendig tidsdekning\n",
    "\n",
    "Vi utviklet en egen rensefil (`data_cleaning_nilu.py`) med tydelig struktur:\n",
    "\n",
    "### ğŸ”§ Rensing av NILU-data â€“ steg for steg\n",
    "\n",
    "1. **Konvertering og sortering av datoer**\n",
    "   - Vi konverterer `dateTime` til `datetime`-objekter og sorterer datasettet kronologisk.\n",
    "   - Dette er nÃ¸dvendig for Ã¥ kunne jobbe med dataserier og bruke KNN-imputasjon senere.\n",
    "\n",
    "2. **Fjerning av lite relevante komponenter**\n",
    "   - Kolonnen `Benzo(a)pyrene in PM10` fjernes fordi den bestÃ¥r nesten utelukkende av manglende verdier og ikke er sentral for vÃ¥r analyse.\n",
    "\n",
    "3. **Outlier-fjerning**\n",
    "   - Med `OutlierValidator` fjerner vi verdier som er mer enn 4 standardavvik fra gjennomsnittet.\n",
    "   - Dette gjÃ¸r vi separat for hver komponent (NOâ‚‚, PM10, PM2.5) for Ã¥ redusere pÃ¥virkning fra ekstremverdier.\n",
    "\n",
    "4. **KNN-imputasjon for manglende verdier**\n",
    "   - Vi bruker `KNNImputer` fra `sklearn` til Ã¥ estimere manglende verdier.\n",
    "   - Imputasjonen baseres pÃ¥ lignende dager i datasettet, noe som gir mer realistiske estimater enn enkle gjennomsnitt.\n",
    "   - Du kan lese mer om dette steget [**her**](../notebooks/KNN_imputation.ipynb).\n",
    "\n",
    "5. **Merking av estimerte verdier**\n",
    "   - Vi legger til egne kolonner (`generated_NO2`, `generated_PM10`, `generated_PM2.5`) som markerer hvilke verdier som er imputert med KNN.\n",
    "   - Dette gir transparens og gjÃ¸r det mulig Ã¥ filtrere estimerte verdier i videre analyser.\n",
    "\n",
    "6. **Datavalidering**\n",
    "   - Vi bruker `MissingValueValidator`, `OutlierValidator` og `DateContinuityValidator` til Ã¥ gi oversikt over datakvalitet fÃ¸r og etter rensing.\n",
    "   - Dette gir dokumentasjon av renseprosessen og innsikt i effekten av hvert steg.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ¦ï¸ Frost â€“ robust, men ikke perfekt\n",
    "\n",
    "Frost-dataene fra MET har hÃ¸y kvalitet, men ble likevel renset og validert med `data_cleaning_frost.py`.\n",
    "\n",
    "### ğŸ”§ HÃ¥ndtering av Frost-data â€“ steg for steg\n",
    "\n",
    "1. **Innlasting og datakonvertering**\n",
    "   - Data hentes fra en rÃ¥ `.json`-fil og konverteres til et strukturert DataFrame-format.\n",
    "   - `referenceTime` konverteres til `datetime` og brukes som indeks for sortering.\n",
    "\n",
    "2. **Filtrering av Ã¸nskede mÃ¥leparametere**\n",
    "   - Vi fokuserer pÃ¥ tre sentrale vÃ¦rvariabler:\n",
    "     - Daglig gjennomsnittstemperatur (`mean_air_temperature`)\n",
    "     - Daglig total nedbÃ¸r (`sum_precipitation_amount`)\n",
    "     - Daglig gjennomsnittlig vindstyrke (`mean_wind_speed`)\n",
    "   - Alle andre mÃ¥letyper ekskluderes.\n",
    "\n",
    "3. **Pivotering og navneforenkling**\n",
    "   - Datasettet transformeres slik at Ã©n rad representerer Ã©n dato, med egne kolonner for hver vÃ¦rkomponent.\n",
    "   - Variabelnavn gjÃ¸res mer lesbare og konsistente.\n",
    "\n",
    "4. **Validering og rensing av verdier**\n",
    "   - Vi bruker `ValueRangeValidator` til Ã¥ fjerne urealistiske mÃ¥linger:\n",
    "     - Temperatur: mellom -30 og 40â€¯Â°C\n",
    "     - NedbÃ¸r: mellom 0 og 250â€¯mm\n",
    "     - Vind: mellom 0 og 60â€¯m/s\n",
    "   - Dette hindrer feilaktige uteliggere i Ã¥ forstyrre analysen.\n",
    "\n",
    "5. **Kontinuitet i datoer**\n",
    "   - `DateContinuityValidator` kontrollerer at det ikke er stÃ¸rre hull i tidsserien.\n",
    "   - Vi logger eventuelle manglende datoer, men imputasjon gjÃ¸res ikke da Frost-dataene er svÃ¦rt komplette.\n",
    "\n",
    "6. **Lagring i SQLite**\n",
    "   - Det rensede datasettet lagres i en SQLite-database (`frost.db`) for videre analyse og prediktiv modellering.\n",
    "   - Dette gjÃ¸r det enkelt Ã¥ koble Frost- og NILU-data senere ved hjelp av SQL.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Hvorfor disse valgene?\n",
    "\n",
    "Valgene vÃ¥re er gjort med tanke pÃ¥:\n",
    "- **Datakvalitet**: Vi Ã¸nsker Ã¥ kunne stole pÃ¥ analysene\n",
    "- **Ã…penhet**: Vi synliggjÃ¸r estimerte data og ekstreme verdier\n",
    "- **Brukervennlighet**: Formatene vi lagrer i er lette Ã¥ bruke i bÃ¥de Python og SQL\n",
    "- **Tilpasning til datasettene**: Frost og NILU krever ulik tilnÃ¦rming â€“ og det har vi tatt hensyn til\n",
    "\n",
    "Ved Ã¥ bruke egne validator-klasser (som `MissingValueValidator`, `OutlierValidator`, `DateContinuityValidator`, `ValueRangeValidator`) har vi laget en fleksibel og gjenbrukbar lÃ¸sning som ogsÃ¥ kan skaleres til andre datatyper.\n",
    "\n",
    "---\n",
    "\n",
    "Vil du se koden som gjÃ¸r alt dette? Ta en titt pÃ¥:\n",
    "- [`data_cleaning_nilu.py`](../src/data_cleaning/data_cleaning_nilu.py)\n",
    "- [`data_cleaning_frost.py`](../src/data_cleaning/data_cleaning_frost.py)\n",
    "- [`data_validators.py`](../src/data_cleaning/data_validators.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c50e0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ğŸŒ«ï¸ Rensing av NILU luftkvalitetsdata\n",
    "\n",
    "NÃ¥ skal vi kjÃ¸re selve koden som renser luftkvalitetsdataene fra NILU. Som beskrevet over, er dette datasettet mer utfordrende med:\n",
    "\n",
    "- Manglende verdier som mÃ¥ hÃ¥ndteres\n",
    "- Outliers som mÃ¥ identifiseres og fjernes\n",
    "- Data som mÃ¥ restruktureres for analyse\n",
    "\n",
    "Vi bruker funksjonen `main_dc_nilu()` som implementerer alle stegene i renseprosessen beskrevet over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e80811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JSON-filen ble lastet vellykket.\n",
      "\n",
      "Dataset informasjon:\n",
      "\n",
      "Antall rader i datasettet: 5222\n",
      "\n",
      "Antall outliers fjernet per verdi:\n",
      "  - NO2: 13 outliers fjernet\n",
      "  - PM10: 52 outliers fjernet\n",
      "  - PM2.5: 45 outliers fjernet\n",
      "\n",
      "Genererte verdier:\n",
      "  - generated_NO2: 1268 genererte verdier\n",
      "  - generated_PM10: 524 genererte verdier\n",
      "  - generated_PM2.5: 532 genererte verdier\n",
      "\n",
      "Konverterer data til JSON format...\n",
      "Renset data lagret i '/Users/oliverroddesnes/Documents/Anvendt_programering/prosjekt_miljodataanalyse/data/clean/cleaned_data_nilu.json'\n",
      "\n",
      "Data rensing fullfÃ¸rt\n"
     ]
    }
   ],
   "source": [
    "main_dc_nilu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9219302",
   "metadata": {},
   "source": [
    "### ğŸŒ¦ï¸ Rensing av Frost vÃ¦rdata\n",
    "\n",
    "NÃ¥ skal vi rense vÃ¦rdataene hentet fra Frost API. Dette datasettet er langt mer strukturert enn NILU-dataene, men det krever fortsatt:\n",
    "\n",
    "- Omstrukturering for Ã¥ fÃ¥ Ã©n rad per dag\n",
    "- Fjerning av urealistiske verdier via validering\n",
    "- Kontroll av datakontinuitet over hele mÃ¥leperioden\n",
    "\n",
    "Vi bruker funksjonen `default_clean_frost_data()` som gjennomfÃ¸rer hele renseprosessen steg for steg â€“ slik det er beskrevet over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91b26f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values detected:\n",
      "mean_wind_speed:\n",
      "- 2012: 1\n",
      "- 2019: 1\n",
      "\n",
      "No outliers detected\n",
      "\n",
      "No date gaps detected\n",
      "\n",
      "Generated values:\n",
      "- mean_wind_speed: 2\n",
      "\n",
      "Cleaned data saved to '/Users/oliverroddesnes/Documents/Anvendt_programering/prosjekt_miljodataanalyse/data/clean/frost.db' in the table 'weather_data'.\n"
     ]
    }
   ],
   "source": [
    "default_clean_frost_data(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467b0c4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Hvorfor dette er en god tilnÃ¦rming\n",
    "\n",
    "Vi har valgt rensestrategier basert pÃ¥ datakildenes egenskaper:\n",
    "\n",
    "- **NILU** krevde:\n",
    "  - Komplett rekonstruering av datastrukturen\n",
    "  - Avansert imputasjon og glatting\n",
    "  - Transparens rundt hvilke verdier som er estimert\n",
    "\n",
    "- **Frost** krevde:\n",
    "  - Kontroll av gyldige verdier og datoer\n",
    "  - Lett justering og tilrettelegging for videre analyse\n",
    "\n",
    "Dette viser:\n",
    "- ForstÃ¥else av hva som pÃ¥virker datakvalitet\n",
    "- Evne til Ã¥ tilpasse metoder etter utfordring\n",
    "- Fokus pÃ¥ transparens og sporbarhet\n",
    "\n",
    "Resultatet er et datasett som bÃ¥de er **ryddet og dokumentert**, og klart for videre analyse, visualisering og modellering.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Neste steg: Analyse og visualisering\n",
    "\n",
    "NÃ¥ som begge datasett er renset og klargjort, er vi klare for Ã¥ utforske innholdet mer inngÃ¥ende. I neste notebook skal vi:\n",
    "\n",
    "- Beregne statistiske mÃ¥l som gjennomsnitt, median og standardavvik\n",
    "- UndersÃ¸ke sammenhenger og trender i luftkvalitet og vÃ¦rdata over tid\n",
    "- Lage visuelle fremstillinger som gjÃ¸r datamÃ¸nstre lettere Ã¥ forstÃ¥\n",
    "\n",
    "\n",
    "### [**Videre til analyse og visualisering**](02_data_analysis_and_visualisation.ipynb)\n",
    "##### [**Til samlesiden**](../docs/samleside.md)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
